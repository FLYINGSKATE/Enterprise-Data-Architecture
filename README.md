# ğŸ—ï¸ StreamCommerce Data Platform - Enterprise Data Architecture

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Apache Airflow](https://img.shields.io/badge/Airflow-2.8+-red.svg)](https://airflow.apache.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A production-ready, scalable data platform for e-commerce analytics demonstrating modern data architecture patterns, real-time streaming, batch processing, and data governance.

## ğŸ¯ Project Overview

**StreamCommerce Data Platform** is a comprehensive data infrastructure that handles:
- ğŸ“Š Real-time clickstream analytics (1M+ events/day)
- ğŸ›’ Transactional data processing (orders, inventory, customers)
- ğŸ“ˆ Business intelligence dashboards
- ğŸ¤– ML feature engineering and model training
- ğŸ”’ Data governance and quality monitoring
- ğŸ“‹ Self-service data catalog

**Key Achievements:**
- âš¡ Sub-second latency for real-time metrics
- ğŸ’¾ Handles 10TB+ data with optimized storage
- ğŸ¯ 99.9% data quality score
- ğŸ“Š 30+ automated dashboards
- ğŸ”„ Complete data lineage tracking
- ğŸš€ Horizontally scalable architecture

## ğŸ›ï¸ Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  STREAMCOMMERCE DATA PLATFORM                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                   â†“                   â†“
   [INGESTION]        [PROCESSING]        [CONSUMPTION]
        â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”
    â”‚ Kafka  â”‚         â”‚ Airflow â”‚         â”‚BI Toolsâ”‚
    â”‚ APIs   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   dbt   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ML APIs â”‚
    â”‚ CDC    â”‚         â”‚ Spark   â”‚         â”‚Catalog â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      [DATA LAKE]
                     Bronzeâ”‚Silverâ”‚Gold
```

### Technology Stack

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Ingestion** | Apache Kafka | Real-time event streaming |
| | Debezium | Change Data Capture (CDC) |
| | Airbyte | Batch data integration |
| **Storage** | MinIO/S3 | Object storage (Data Lake) |
| | PostgreSQL | Data Warehouse |
| | DuckDB | Analytics engine |
| **Processing** | Apache Airflow | Workflow orchestration |
| | dbt | SQL transformations |
| | Apache Spark | Big data processing |
| | Kafka Streams | Stream processing |
| **Quality** | Great Expectations | Data validation |
| | dbt tests | Transform tests |
| **Governance** | DataHub | Data catalog |
| | Apache Atlas | Metadata management |
| **Analytics** | Metabase/Superset | BI dashboards |
| | Feast | Feature store |
| | FastAPI | Data APIs |
| **Monitoring** | Prometheus | Metrics collection |
| | Grafana | Visualization |
| **Infrastructure** | Docker | Containerization |
| | Docker Compose | Local orchestration |
| | Terraform | Infrastructure as Code |

## ğŸ“ Project Structure

```
streamcommerce-data-platform/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Raw data samples
â”‚   â”œâ”€â”€ bronze/                       # Unprocessed data lake layer
â”‚   â”œâ”€â”€ silver/                       # Cleaned data lake layer
â”‚   â””â”€â”€ gold/                         # Business-level aggregations
â”‚
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ diagrams/                     # Architecture diagrams
â”‚   â”‚   â”œâ”€â”€ high_level_architecture.png
â”‚   â”‚   â”œâ”€â”€ data_flow_diagram.png
â”‚   â”‚   â”œâ”€â”€ erd_transactional.png
â”‚   â”‚   â””â”€â”€ dimensional_model.png
â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ ARCHITECTURE.md           # Detailed architecture
â”‚   â”‚   â”œâ”€â”€ DATA_MODEL.md             # Data modeling docs
â”‚   â”‚   â”œâ”€â”€ DESIGN_DECISIONS.md       # ADR (Architecture Decision Records)
â”‚   â”‚   â””â”€â”€ SCALING_STRATEGY.md       # Scalability approach
â”‚   â””â”€â”€ templates/
â”‚       â”œâ”€â”€ data_dictionary_template.xlsx
â”‚       â””â”€â”€ data_lineage_template.xlsx
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â”œâ”€â”€ producers/            # Event producers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clickstream_producer.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ order_producer.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ inventory_producer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ consumers/            # Event consumers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ clickstream_consumer.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ real_time_aggregator.py
â”‚   â”‚   â”‚   â””â”€â”€ schemas/              # Avro/JSON schemas
â”‚   â”‚   â”‚       â”œâ”€â”€ clickstream.avsc
â”‚   â”‚   â”‚       â””â”€â”€ order.avsc
â”‚   â”‚   â”œâ”€â”€ cdc/
â”‚   â”‚   â”‚   â”œâ”€â”€ debezium_config.json  # CDC configuration
â”‚   â”‚   â”‚   â””â”€â”€ sync_transactional_db.py
â”‚   â”‚   â””â”€â”€ api_connectors/
â”‚   â”‚       â”œâ”€â”€ stripe_connector.py
â”‚   â”‚       â”œâ”€â”€ shippo_connector.py
â”‚   â”‚       â””â”€â”€ product_api_connector.py
â”‚   â”‚
â”‚   â”œâ”€â”€ data_lake/
â”‚   â”‚   â”œâ”€â”€ lake_manager.py           # Data lake operations
â”‚   â”‚   â”œâ”€â”€ partition_manager.py      # Partitioning logic
â”‚   â”‚   â””â”€â”€ medallion_architecture.py # Bronze/Silver/Gold layers
â”‚   â”‚
â”‚   â”œâ”€â”€ warehouse/
â”‚   â”‚   â”œâ”€â”€ models/                   # SQL data models
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/              # Raw â†’ staging
â”‚   â”‚   â”‚   â”œâ”€â”€ intermediate/         # Business logic
â”‚   â”‚   â”‚   â””â”€â”€ marts/                # Final tables
â”‚   â”‚   â”œâ”€â”€ schema.sql                # DDL scripts
â”‚   â”‚   â””â”€â”€ migrations/               # Schema migrations
â”‚   â”‚
â”‚   â”œâ”€â”€ transformations/
â”‚   â”‚   â”œâ”€â”€ dbt_project/              # dbt transformations
â”‚   â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”‚   â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”‚   â”œâ”€â”€ macros/
â”‚   â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â”‚   â””â”€â”€ spark_jobs/
â”‚   â”‚       â”œâ”€â”€ customer_segmentation.py
â”‚   â”‚       â”œâ”€â”€ product_recommendations.py
â”‚   â”‚       â””â”€â”€ cohort_analysis.py
â”‚   â”‚
â”‚   â”œâ”€â”€ quality/
â”‚   â”‚   â”œâ”€â”€ expectations/             # Great Expectations suites
â”‚   â”‚   â”‚   â”œâ”€â”€ orders_expectations.json
â”‚   â”‚   â”‚   â”œâ”€â”€ customers_expectations.json
â”‚   â”‚   â”‚   â””â”€â”€ products_expectations.json
â”‚   â”‚   â”œâ”€â”€ data_quality_checks.py
â”‚   â”‚   â””â”€â”€ anomaly_detection.py
â”‚   â”‚
â”‚   â”œâ”€â”€ governance/
â”‚   â”‚   â”œâ”€â”€ catalog/
â”‚   â”‚   â”‚   â”œâ”€â”€ datahub_ingestion.py
â”‚   â”‚   â”‚   â””â”€â”€ metadata_sync.py
â”‚   â”‚   â”œâ”€â”€ lineage/
â”‚   â”‚   â”‚   â”œâ”€â”€ lineage_tracker.py
â”‚   â”‚   â”‚   â””â”€â”€ impact_analysis.py
â”‚   â”‚   â”œâ”€â”€ access_control/
â”‚   â”‚   â”‚   â”œâ”€â”€ rbac_policies.yaml
â”‚   â”‚   â”‚   â””â”€â”€ data_masking.py
â”‚   â”‚   â””â”€â”€ compliance/
â”‚   â”‚       â”œâ”€â”€ gdpr_compliance.py
â”‚   â”‚       â”œâ”€â”€ pii_detector.py
â”‚   â”‚       â””â”€â”€ retention_policies.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analytics/
â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”‚   â”œâ”€â”€ executive_dashboard.sql
â”‚   â”‚   â”‚   â”œâ”€â”€ sales_analytics.sql
â”‚   â”‚   â”‚   â””â”€â”€ customer_insights.sql
â”‚   â”‚   â”œâ”€â”€ reports/
â”‚   â”‚   â”‚   â”œâ”€â”€ daily_kpi_report.py
â”‚   â”‚   â”‚   â””â”€â”€ weekly_summary.py
â”‚   â”‚   â””â”€â”€ ml_features/
â”‚   â”‚       â”œâ”€â”€ feature_engineering.py
â”‚   â”‚       â”œâ”€â”€ feature_store_sync.py
â”‚   â”‚       â””â”€â”€ feast_features/
â”‚   â”‚
â”‚   â”œâ”€â”€ apis/
â”‚   â”‚   â”œâ”€â”€ data_api.py               # FastAPI data service
â”‚   â”‚   â”œâ”€â”€ metrics_api.py
â”‚   â”‚   â””â”€â”€ query_api.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py
â”‚       â”œâ”€â”€ logging_config.py
â”‚       â”œâ”€â”€ database_utils.py
â”‚       â””â”€â”€ helpers.py
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ daily_etl_pipeline.py
â”‚   â”‚   â”œâ”€â”€ hourly_aggregations.py
â”‚   â”‚   â”œâ”€â”€ weekly_reports.py
â”‚   â”‚   â”œâ”€â”€ data_quality_checks.py
â”‚   â”‚   â””â”€â”€ ml_feature_pipeline.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â”‚   â”œâ”€â”€ custom_operators/
â”‚   â”‚   â””â”€â”€ custom_sensors/
â”‚   â””â”€â”€ config/
â”‚       â””â”€â”€ airflow.cfg
â”‚
â”œâ”€â”€ infrastructure/
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ kafka/
â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ airflow/
â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ postgres/
â”‚   â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ monitoring/
â”‚   â”‚       â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml            # Local development stack
â”‚   â”œâ”€â”€ docker-compose.prod.yml       # Production stack
â”‚   â”œâ”€â”€ terraform/                    # Cloud infrastructure
â”‚   â”‚   â”œâ”€â”€ aws/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.tf
â”‚   â”‚   â”‚   â”œâ”€â”€ variables.tf
â”‚   â”‚   â”‚   â””â”€â”€ outputs.tf
â”‚   â”‚   â””â”€â”€ gcp/
â”‚   â””â”€â”€ kubernetes/                   # K8s manifests
â”‚       â”œâ”€â”€ deployments/
â”‚       â”œâ”€â”€ services/
â”‚       â””â”€â”€ configmaps/
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ prometheus/
â”‚   â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”‚   â””â”€â”€ alerts.yml
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â”œâ”€â”€ dashboards/
â”‚   â”‚   â”‚   â”œâ”€â”€ data_pipeline_health.json
â”‚   â”‚   â”‚   â”œâ”€â”€ kafka_metrics.json
â”‚   â”‚   â”‚   â””â”€â”€ warehouse_performance.json
â”‚   â”‚   â””â”€â”€ provisioning/
â”‚   â””â”€â”€ logs/
â”‚       â””â”€â”€ loki_config.yml
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_data_generation.ipynb      # Generate sample data
â”‚   â”œâ”€â”€ 02_data_profiling.ipynb       # EDA and profiling
â”‚   â”œâ”€â”€ 03_architecture_design.ipynb  # Design decisions
â”‚   â”œâ”€â”€ 04_ingestion_setup.ipynb      # Kafka setup
â”‚   â”œâ”€â”€ 05_warehouse_modeling.ipynb   # Data modeling
â”‚   â”œâ”€â”€ 06_dbt_transformations.ipynb  # Transform development
â”‚   â”œâ”€â”€ 07_quality_framework.ipynb    # Quality checks
â”‚   â”œâ”€â”€ 08_analytics_demo.ipynb       # BI queries
â”‚   â””â”€â”€ 09_governance_setup.ipynb     # Catalog & lineage
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”‚   â”œâ”€â”€ test_transformations.py
â”‚   â”‚   â””â”€â”€ test_quality.py
â”‚   â”œâ”€â”€ integration/
â”‚   â”‚   â”œâ”€â”€ test_pipeline_e2e.py
â”‚   â”‚   â””â”€â”€ test_data_flow.py
â”‚   â””â”€â”€ performance/
â”‚       â”œâ”€â”€ test_query_performance.py
â”‚       â””â”€â”€ load_testing.py
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ setup_environment.sh
â”‚   â”‚   â”œâ”€â”€ initialize_databases.sh
â”‚   â”‚   â””â”€â”€ seed_data.py
â”‚   â”œâ”€â”€ deployment/
â”‚   â”‚   â”œâ”€â”€ deploy_local.sh
â”‚   â”‚   â””â”€â”€ deploy_production.sh
â”‚   â””â”€â”€ utilities/
â”‚       â”œâ”€â”€ backup_databases.sh
â”‚       â”œâ”€â”€ reset_environment.sh
â”‚       â””â”€â”€ generate_sample_events.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ setup/
â”‚   â”‚   â”œâ”€â”€ INSTALLATION.md
â”‚   â”‚   â””â”€â”€ QUICKSTART.md
â”‚   â”œâ”€â”€ architecture/
â”‚   â”‚   â”œâ”€â”€ OVERVIEW.md
â”‚   â”‚   â”œâ”€â”€ DATA_FLOW.md
â”‚   â”‚   â”œâ”€â”€ TECHNOLOGY_CHOICES.md
â”‚   â”‚   â””â”€â”€ SCALING.md
â”‚   â”œâ”€â”€ development/
â”‚   â”‚   â”œâ”€â”€ CONTRIBUTING.md
â”‚   â”‚   â”œâ”€â”€ CODING_STANDARDS.md
â”‚   â”‚   â””â”€â”€ TESTING.md
â”‚   â”œâ”€â”€ operations/
â”‚   â”‚   â”œâ”€â”€ DEPLOYMENT.md
â”‚   â”‚   â”œâ”€â”€ MONITORING.md
â”‚   â”‚   â””â”€â”€ TROUBLESHOOTING.md
â”‚   â””â”€â”€ governance/
â”‚       â”œâ”€â”€ DATA_CATALOG.md
â”‚       â”œâ”€â”€ QUALITY_STANDARDS.md
â”‚       â””â”€â”€ COMPLIANCE.md
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dev.env
â”‚   â”œâ”€â”€ prod.env
â”‚   â”œâ”€â”€ kafka_config.yaml
â”‚   â”œâ”€â”€ warehouse_config.yaml
â”‚   â””â”€â”€ quality_thresholds.yaml
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ ci.yml
â”‚       â””â”€â”€ deploy.yml
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose
- Python 3.10+
- 16GB RAM minimum
- 50GB free disk space

### Installation

```bash
# Clone repository
git clone https://github.com/yourusername/streamcommerce-platform.git
cd streamcommerce-platform

# Setup environment
make setup

# Start infrastructure
make start-infra

# Initialize databases
make init-db

# Seed sample data
make seed-data

# Start Airflow
make start-airflow
```

### Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| Airflow | http://localhost:8080 | admin/admin |
| Metabase | http://localhost:3000 | Setup on first visit |
| Kafka UI | http://localhost:8081 | - |
| Grafana | http://localhost:3001 | admin/admin |
| MinIO | http://localhost:9001 | minioadmin/minioadmin |
| DataHub | http://localhost:9002 | datahub/datahub |

## ğŸ“Š Data Model

### Transactional Database (OLTP)

**Entity-Relationship Diagram:**

```
CUSTOMERS                    ORDERS                      PRODUCTS
â”œâ”€â”€ customer_id (PK)        â”œâ”€â”€ order_id (PK)          â”œâ”€â”€ product_id (PK)
â”œâ”€â”€ email                   â”œâ”€â”€ customer_id (FK)       â”œâ”€â”€ name
â”œâ”€â”€ name                    â”œâ”€â”€ order_date             â”œâ”€â”€ category
â”œâ”€â”€ phone                   â”œâ”€â”€ status                 â”œâ”€â”€ price
â”œâ”€â”€ address                 â”œâ”€â”€ total_amount           â”œâ”€â”€ stock_quantity
â””â”€â”€ created_at              â””â”€â”€ shipping_address       â””â”€â”€ created_at
        â”‚                           â”‚                          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                              ORDER_ITEMS
                              â”œâ”€â”€ item_id (PK)
                              â”œâ”€â”€ order_id (FK)
                              â”œâ”€â”€ product_id (FK)
                              â”œâ”€â”€ quantity
                              â””â”€â”€ unit_price
```

### Data Warehouse (OLAP) - Star Schema

```
        DIM_CUSTOMER              DIM_PRODUCT
        â”œâ”€â”€ customer_key          â”œâ”€â”€ product_key
        â”œâ”€â”€ customer_id           â”œâ”€â”€ product_id
        â”œâ”€â”€ name                  â”œâ”€â”€ name
        â”œâ”€â”€ segment               â”œâ”€â”€ category
        â””â”€â”€ lifetime_value        â””â”€â”€ subcategory
                 â”‚                         â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                     FACT_SALES
                     â”œâ”€â”€ sales_key
                     â”œâ”€â”€ date_key (FK)
                     â”œâ”€â”€ customer_key (FK)
                     â”œâ”€â”€ product_key (FK)
                     â”œâ”€â”€ quantity
                     â”œâ”€â”€ revenue
                     â”œâ”€â”€ cost
                     â””â”€â”€ profit
                          â”‚
                     DIM_DATE
                     â”œâ”€â”€ date_key
                     â”œâ”€â”€ date
                     â”œâ”€â”€ day_of_week
                     â”œâ”€â”€ month
                     â”œâ”€â”€ quarter
                     â””â”€â”€ year
```

## ğŸ”„ Data Pipeline Architecture

### Real-Time Stream Processing

```mermaid
graph LR
    A[Website] -->|Events| B[Kafka]
    B --> C[Stream Processor]
    C --> D[Real-time Metrics]
    C --> E[Data Lake Bronze]
```

### Batch ETL Pipeline

```mermaid
graph TD
    A[Source Systems] -->|Extract| B[Staging]
    B -->|Transform dbt| C[Data Warehouse]
    C -->|Aggregate| D[Data Marts]
    D -->|Serve| E[BI Tools]
    D -->|Serve| F[ML Models]
```

## ğŸ“ˆ Key Features

### 1. Medallion Architecture (Bronze/Silver/Gold)

**Bronze Layer** (Raw Data)
- Exact copy of source data
- Immutable
- Partitioned by date
- Compressed (Parquet)

**Silver Layer** (Cleaned Data)
- Validated and cleansed
- Deduplicated
- Type-safe
- Schema enforced

**Gold Layer** (Business-Level)
- Aggregated metrics
- Business entities
- Optimized for queries
- Powers dashboards

### 2. Real-Time Analytics

- Sub-second latency for key metrics
- Live dashboards update every 5 seconds
- Streaming aggregations (windowing)
- Real-time alerts on anomalies

### 3. Data Quality Framework

```python
# Example: Order validation expectations
expectations = {
    "order_amount": {
        "min": 0,
        "max": 100000,
        "never_null": True
    },
    "order_status": {
        "values_in": ["pending", "paid", "shipped", "delivered"]
    },
    "email": {
        "regex": "^[a-zA-Z0-9+_.-]+@[a-zA-Z0-9.-]+$"
    }
}
```

### 4. Data Governance

**Data Catalog:**
- Searchable metadata
- Business glossary
- Column-level descriptions
- Data owners
- Usage statistics

**Data Lineage:**
- Source-to-dashboard tracking
- Impact analysis
- Dependency mapping

**Access Control:**
- Role-based access (RBAC)
- Column-level security
- Row-level security
- Audit logging

## ğŸ“Š Sample Dashboards

### Executive Dashboard
- Daily Revenue
- Order Conversion Rate
- Customer Acquisition Cost
- Average Order Value
- Top Products

### Sales Analytics
- Revenue by Category
- Sales Trends
- Geographic Distribution
- Sales Funnel

### Customer Insights
- Customer Segmentation
- Cohort Analysis
- Churn Prediction
- Lifetime Value

## ğŸ” Monitoring & Observability

### Data Pipeline Health

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| Pipeline Success Rate | >99% | <95% |
| Data Freshness | <15 min | >30 min |
| Data Quality Score | >98% | <95% |
| Query Performance (p95) | <2s | >5s |
| Storage Growth | <10% week | >20% week |

### Infrastructure Metrics

- Kafka lag
- Database connections
- CPU/Memory usage
- Disk I/O
- Network throughput

## ğŸ“ Learning Outcomes

This project demonstrates:

âœ… End-to-end data architecture design  
âœ… Modern data stack implementation  
âœ… Real-time & batch processing  
âœ… Data modeling (OLTP & OLAP)  
âœ… ETL/ELT pipelines  
âœ… Data quality & governance  
âœ… Infrastructure as code  
âœ… Cloud-ready architecture  
âœ… Performance optimization  
âœ… Production best practices  

## ğŸ› ï¸ Technology Deep-Dive

### Why These Tools?

| Tool | Why Chosen | Alternatives |
|------|-----------|--------------|
| Kafka | Industry standard for streaming | AWS Kinesis, Pulsar |
| Airflow | Best workflow orchestration | Prefect, Dagster |
| dbt | SQL-first transformations | Dataform, SQLMesh |
| PostgreSQL | Reliable, feature-rich | MySQL, Oracle |
| MinIO | S3-compatible, self-hosted | AWS S3, GCS |
| Metabase | Open-source, easy setup | Superset, Tableau |

## ğŸ“š Documentation

- [Architecture Overview](docs/architecture/OVERVIEW.md)
- [Setup Guide](docs/setup/INSTALLATION.md)
- [Data Model Documentation](docs/architecture/DATA_FLOW.md)
- [Operations Guide](docs/operations/DEPLOYMENT.md)
- [API Reference](docs/api/API_REFERENCE.md)

## ğŸ¯ Future Enhancements

- [ ] Add Apache Iceberg for table format
- [ ] Implement CDC for all sources
- [ ] Add machine learning pipelines
- [ ] Multi-region replication
- [ ] Advanced security (encryption at rest)
- [ ] Cost optimization framework
- [ ] Self-healing pipelines
- [ ] Natural language query interface

## ğŸ¤ Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md)

## ğŸ“ License

MIT License - see [LICENSE](LICENSE)

## ğŸ™ Acknowledgments

- Apache Software Foundation
- dbt Labs
- Confluent (Kafka)
- Modern Data Stack community

---

**â­ Star this repo if it helped you learn data architecture!**
